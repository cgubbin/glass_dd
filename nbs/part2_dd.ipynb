{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "001fe114",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Reload\n",
    "\n",
    "This is just some code from the first part. This assumes that the dataset was downloaded as in the first notebook and that the COCO format Json files were generated. We continue to work with the three-channel composite images generated in part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ab5757",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cmx\n",
    "from matplotlib import patches, patheffects\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#\n",
    "SIZE=512\n",
    "\n",
    "path = Path.cwd() / \"data\"\n",
    "\n",
    "# display an image in such a way that we can layer on some additional annotations\n",
    "def show_img(im, figsize=None, ax=None):\n",
    "    if not ax: fig,ax = plt.subplots(figsize=figsize)\n",
    "    show_image(im,ax) # We use this FastAI method to make life a little easier\n",
    "    ax.set_xticks(np.linspace(0, SIZE, 8))\n",
    "    ax.set_yticks(np.linspace(0, SIZE, 8))\n",
    "    ax.grid()\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "    return ax\n",
    "\n",
    "# draw an outline around the shape; used to add contrast to the text so we can read it easily\n",
    "def draw_outline(o, lw):\n",
    "    o.set_path_effects([patheffects.Stroke(\n",
    "        linewidth=lw, foreground='black'), patheffects.Normal()])\n",
    "\n",
    "# draw text in the specified location along with an outline so that there is some contrast between the text and the image\n",
    "def draw_text(ax, xy, txt, sz=14, color='white'):\n",
    "    text = ax.text(*xy, txt,\n",
    "        verticalalignment='top', color=color, fontsize=sz, weight='bold')\n",
    "    draw_outline(text, 1)\n",
    "\n",
    "def draw_rect(ax, b, color='white'):\n",
    "    patch = ax.add_patch(patches.Rectangle(b[:2], *b[-2:], fill=False, edgecolor=color, lw=2))\n",
    "    draw_outline(patch, 4)\n",
    "\n",
    "def get_cmap(N):\n",
    "    color_norm  = mcolors.Normalize(vmin=0, vmax=N-1)\n",
    "    return cmx.ScalarMappable(norm=color_norm, cmap='Set3').to_rgba\n",
    "\n",
    "# generate a list of different colors for rendering our bounding boxes\n",
    "num_colr = 12\n",
    "cmap = get_cmap(num_colr)\n",
    "colr_list = [cmap(float(x)) for x in range(num_colr)]\n",
    "\n",
    "# Grab our dataset\n",
    "train_imgs, train_lbl_bbox = get_annotations(path/'composite'/'train'/'data.json')\n",
    "valid_imgs, valid_lbl_bbox = get_annotations(path/'composite'/'val'/'data.json')\n",
    "imgs = [f\"train/{each}\" for each in train_imgs] + [f\"val/{each}\" for each in valid_imgs]\n",
    "lbl_bbox = train_lbl_bbox + valid_lbl_bbox\n",
    "\n",
    "# utility function that takes a bounding box in the form of x1,y1,x2,y2 and returns it's area (w*h)\n",
    "def area(b): return (b[2]-b[0])*(b[3]-b[1])\n",
    "# zip the bounding boxes together with the object class; sort it descending order by the size of the bounding; return the first one (largest one)\n",
    "def get_largest(boxes):\n",
    "    return sorted(L(zip(*boxes)),key=lambda b: -area(b[0]))[0]\n",
    "    \n",
    "# list comprehension to go through all of the training data and extract the largest objects\n",
    "lrg_bbox = [get_largest(boxes) for boxes in lbl_bbox]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc289f3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "This takes us back to the point where we had a \"largest object dataset\" which for a given image contains the largest object, and its bounding box.\n",
    "\n",
    "# Scaling\n",
    "In this part we are going to look in more depth at the bounding boxes. As these are defined in terms of image coordinates, and we scale images down to `SIZE` we also need to scale the bounding boxes. For now we can do this in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d884839",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Given the image file name and bounding box will scale the bounding box to 224x224\n",
    "def squish_bbox(img_file,labeled_bbox):\n",
    "    bbox,label = labeled_bbox\n",
    "    p = path/\"composite\"/img_file\n",
    "    img = Image.open(p)\n",
    "    h,w = img.shape\n",
    "    yscale,xscale = h/SIZE,w/SIZE\n",
    "    scaled_bbox = (bbox[0]//xscale,bbox[1]//yscale,bbox[2]//xscale,bbox[3]//yscale)\n",
    "    return [scaled_bbox,label]\n",
    "lrg_bbox[1], squish_bbox(imgs[1], lrg_bbox[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df067b6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Now we can scale all the bounding boxes in the \"largest\" dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ad6dff",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "lrg_bbox_scaled = [squish_bbox(img_file, labeled_bbox) for img_file, labeled_bbox in zip(imgs, lrg_bbox)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdff4fac",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We can make a `dict` so the dependent variables can be easily recalled given the independent image file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73f9806",
   "metadata": {},
   "outputs": [],
   "source": [
    "img2lrgbboxscaled = dict(zip(imgs, lrg_bbox_scaled))\n",
    "k = L(img2lrgbboxscaled)[1]; k, img2lrgbboxscaled[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5711dd92",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Visualisation\n",
    "Now we ignore class labels, and just work with the bounding box for the largest class present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceb2309",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def show_lrg_item(im, lbl_bbox, figsize=None, ax=None):\n",
    "    if not ax: fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax = show_img(im, ax=ax)\n",
    "    b = lbl_bbox[0]\n",
    "    b = (*b[:2],b[2]-b[0]+1,b[3]-b[1]+1)\n",
    "    draw_rect(ax, b, color=colr_list[0])\n",
    "\n",
    "img_file = L(img2lrgbboxscaled)[1]\n",
    "img_bbox_scaled = img2lrgbboxscaled[k]\n",
    "\n",
    "img_scaled = Image.open(path/'composite'/img_file).resize((SIZE, SIZE))\n",
    "show_lrg_item(img_scaled, img_bbox_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9e669f",
   "metadata": {},
   "source": [
    "# Training a BB Prediction Model\n",
    "When we made the prior model to classify images by the largest object, fastai itself did a lot. It constructed a model with the resnet34 architecture, and initialised the parameters with a set of pretrained weights. These do *transfer learning*, and are obtained by training the model on the imagenet dataset. This has over 100k images across 200 classes. The early layers of the model are great for detecting edges or primitive shapes common to any images. This start point gives a much more efficient descent than using random initialisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3636b34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the getter that will obtain the input image and the class label associated with the image.\n",
    "getters = [lambda o: path/'composite'/o, lambda o: img2lrgbboxscaled[o][1]]\n",
    "\n",
    "item_tfms = [Resize(SIZE, method='squish'),]\n",
    "batch_tfms = [Rotate(10), Flip(), Dihedral()]\n",
    "dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n",
    "                   getters=getters,\n",
    "                   item_tfms=item_tfms,\n",
    "                   batch_tfms=batch_tfms,\n",
    "                   splitter = FuncSplitter(lambda o: Path(o).parent == 'val'))\n",
    "dls = dblock.dataloaders(imgs, bs = 128)\n",
    "learn = vision_learner(dls,resnet34)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc72aba",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "As we have different class labels in this dataset the latter layers are less useful. FastAI's `vision_learner` adapts the model to our dataset by cutting it into two parts. The pretrained part is retained, and referred to as the backbone of our network. The latter part (the head) is discarded, and replaced with a dynamically created sequence of linear layer with twenty output activations to match the number of classes we want to predict. The head is randomly initialised and trained to map from the backbone features to desired model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d8a921",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "backbone = learn.model[0]; backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06f163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "head = learn.model[1]; head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b41dba",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "The most interesting thing here is the last layer of the head. The `out_features` give one activation for every one of the twenty classes. When we train with CrossEntropy loss the model drives the twenty activations to output probabilities for each of the twenty classes. \n",
    "\n",
    "The bounding box problem is distinct. The bounding box contains for numbers, the first two contain the upper-left coordinates of the rectangle and the latter two the lower-right coordinates. The network now has to output four continuous numbers. To get continuous numbers we need to use regression. We can continue to use the same `resnet34` backbone but the head needs to change.\n",
    "\n",
    "## Bounding Box Model\n",
    "We define a getter for training the model, which given an image file name returns the full path and a tensor with the four bounding box coordinates of the largest object in the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f6f61b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "getters = [lambda o: path/'composite'/o, lambda o: FloatTensor(img2lrgbboxscaled[o][0])]\n",
    "k, getters[0](k), getters[1](k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4f37ad",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "In computer vision we often want to randomly transform the training images by rotation or flipping to increase diversity in the input images and reduce the likelihood of overfitting on the training data. If we do that now we would also need to do the same transformations on the bounding boxes. We neglect that for now, building a model without any image transforms or augmentations. This will be improved below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fbd241",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "item_tfms = [Resize(SIZE, method=\"squish\"),]\n",
    "dblock = DataBlock(blocks=(ImageBlock, RegressionBlock(n_out=4)),\n",
    "                   getters=getters,\n",
    "                   item_tfms=item_tfms,\n",
    "                   splitter = FuncSplitter(lambda o: Path(o).parent == 'val'))\n",
    "dls = dblock.dataloaders(imgs, bs = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54dedb1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Now we can look at a sample batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2178b5c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8db090",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Now for FastAI the bounding box edges are just numbers. We can visualise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87477fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bb_hw(a): return np.array([a[0], a[1], a[2]-a[0]+1, a[3]-a[1]+1])\n",
    "\n",
    "def show_batch(dls):\n",
    "    b = dls.one_batch()\n",
    "\n",
    "    axs = subplots(3, 3)[1].flat\n",
    "    for img, box, ax in zip(b[0][:9], b[1][:9], axs):\n",
    "        show_img(img, ax=ax)\n",
    "        draw_rect(ax, bb_hw(box.tolist()), color=colr_list[0])\n",
    "\n",
    "show_batch(dls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f053ba",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Again we use the vision learner API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795bf617",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "learn = vision_learner(dls, resnet34, loss_func=L1LossFlat())\n",
    "head = learn.model[1]; head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b863246",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "The default loss for regression is MSE, but this heavily penalises larger differences between predictions and targets. An L1 is probably more appropriate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3281cdc0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "lrs = learn.lr_find()\n",
    "lrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aabe4c6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Training\n",
    "We use the fine tune method as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2300a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fine_tune(30, base_lr=lrs.valley)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b5a148",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Evaluation\n",
    "Now lets look at the predictions and see how it's doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00ac4e4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def show_preds(learner,valid=False):\n",
    "    learn.model.eval()\n",
    "    dl = learn.dls[1 if valid else 0].new(shuffle=True)\n",
    "    imgs,boxes = dl.one_batch()\n",
    "    preds = learner.model(imgs).tolist()\n",
    "\n",
    "    fig,axs = subplots(9,2)\n",
    "    for img,box,pred,ax in zip(imgs,boxes,preds,axs):\n",
    "        img = (img-img.min())/(img.max()-img.min())\n",
    "        show_img(img,ax=ax[0])\n",
    "        draw_rect(ax[0],bb_hw(box.tolist()),color=colr_list[0])\n",
    "        show_img(img,ax=ax[1])\n",
    "        draw_rect(ax[1],bb_hw(pred),color=colr_list[0])\n",
    "    fig.tight_layout()\n",
    "    axs[0][0].set_title('label')\n",
    "    axs[0][1].set_title('prediction')\n",
    "    plt.show()\n",
    "\n",
    "show_preds(learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48e80eb",
   "metadata": {},
   "source": [
    "These results are quite reasonable!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
