{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "122731d4",
   "metadata": {},
   "source": [
    "# Predicting Bounding Boxes and Classifications\n",
    "\n",
    "We start by pulling in some code from Part 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddefc535",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cmx\n",
    "from matplotlib import patches, patheffects\n",
    "\n",
    "SIZE=224\n",
    "\n",
    "# display an image in such a way that we can layer on some additional annotations\n",
    "def show_img(im, figsize=None, ax=None):\n",
    "    if not ax: fig,ax = plt.subplots(figsize=figsize)\n",
    "    show_image(im,ax) # We use this FastAI method to make life a little easier\n",
    "    ax.set_xticks(np.linspace(0, SIZE, 8))\n",
    "    ax.set_yticks(np.linspace(0, SIZE, 8))\n",
    "    ax.grid()\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "    return ax\n",
    "\n",
    "# draw an outline around the shape; used to add contrast to the text so we can read it easily\n",
    "def draw_outline(o, lw):\n",
    "    o.set_path_effects([patheffects.Stroke(\n",
    "        linewidth=lw, foreground='black'), patheffects.Normal()])\n",
    "\n",
    "# draw text in the specified location along with an outline so that there is some contrast between the text and the image\n",
    "def draw_text(ax, xy, txt, sz=14, color='white'):\n",
    "    text = ax.text(*xy, txt,\n",
    "        verticalalignment='top', color=color, fontsize=sz, weight='bold')\n",
    "    draw_outline(text, 1)\n",
    "\n",
    "def draw_rect(ax, b, color='white'):\n",
    "    patch = ax.add_patch(patches.Rectangle(b[:2], *b[-2:], fill=False, edgecolor=color, lw=2))\n",
    "    draw_outline(patch, 4)\n",
    "\n",
    "def get_cmap(N):\n",
    "    color_norm  = mcolors.Normalize(vmin=0, vmax=N-1)\n",
    "    return cmx.ScalarMappable(norm=color_norm, cmap='Set3').to_rgba\n",
    "\n",
    "# generate a list of different colors for rendering our bounding boxes\n",
    "num_colr = 12\n",
    "cmap = get_cmap(num_colr)\n",
    "colr_list = [cmap(float(x)) for x in range(num_colr)]\n",
    "\n",
    "# Grab our dataset\n",
    "path = untar_data(URLs.PASCAL_2007)\n",
    "imgs,lbl_bbox = get_annotations(path/'train.json')\n",
    "\n",
    "# utility function that takes a bounding box in the form of x1,y1,x2,y2 and returns it's area (w*h)\n",
    "def area(b): return (b[2]-b[0])*(b[3]-b[1])\n",
    "\n",
    "# zip the bounding boxes together with the object class; sort it descending order by the size of the bounding; return the first one (largest one)\n",
    "def get_largest(boxes):\n",
    "    return sorted(L(zip(*boxes)),key=lambda b: -area(b[0]))[0]\n",
    "    \n",
    "# list comprehension to go through all of the training data and extract the largest objects\n",
    "lrg_bbox = [get_largest(boxes) for boxes in lbl_bbox]\n",
    "# Given the image file name and bounding box will scale the bounding box to 224x224\n",
    "def squish_bbox(img_file,labeled_bbox):\n",
    "    bbox,label = labeled_bbox\n",
    "    p = path/f'train/{img_file}'\n",
    "    img = Image.open(p)\n",
    "    h,w = img.shape\n",
    "    yscale,xscale = h/SIZE,w/SIZE\n",
    "    scaled_bbox = (bbox[0]//xscale,bbox[1]//yscale,bbox[2]//xscale,bbox[3]//yscale)\n",
    "    return [scaled_bbox,label]\n",
    "    \n",
    "lrg_bbox_scaled = [squish_bbox(img_file,labeled_bbox) for img_file,labeled_bbox in zip(imgs,lrg_bbox)]\n",
    "\n",
    "# scale all of the bounding boxes in our 'largest' dataset using a list comprehension.\n",
    "img2lrgbboxscaled = dict(zip(imgs,lrg_bbox_scaled))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba95980",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "This again gives a largest object dataset, along with appropriately scaled bounding boxes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3258be",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = L(img2lrgbboxscaled)[1]; k, img2lrgbboxscaled[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b27965",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Visualisation\n",
    "\n",
    "Let's start by looking at an image from the dataset, along with the bounding box and label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331ace90",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def show_lrg_item(im, lbl_bbox, figsize=None, ax=None):\n",
    "    if not ax: fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax = show_img(im, ax=ax)\n",
    "    b = lbl_bbox[0]\n",
    "    l = lbl_bbox[1]\n",
    "    b = (*b[:2], b[2]-b[0]+1, b[3]-b[1]+1)\n",
    "    draw_rect(ax, b, color=colr_list[0])\n",
    "    draw_text(ax, b[:2], l, color=colr_list[0])\n",
    "\n",
    "img_file = L(img2lrgbboxscaled)[1]\n",
    "img_bbox_scaled = img2lrgbboxscaled[k]\n",
    "\n",
    "img_scaled = Image.open(path/'train'/img_file).resize((SIZE, SIZE))\n",
    "show_lrg_item(img_scaled, img_bbox_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4402103",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Predicting Multiple Things\n",
    "Now we want the bounding box, and the class label. The model needs to be able to access both during training and validation. \n",
    "\n",
    "To create the `DataLoaders` we need an array of getter functions which return the appropriate data. All the functions in the getters take the image file name as input. The first returns the full path, the second returns a tensor containing the four bounding box coordinates and the third returns the string class label for the largest object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175c09b5",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "getters = [\n",
    "    lambda o: path/'train'/o,\n",
    "    lambda o: FloatTensor(img2lrgbboxscaled[o][0]),\n",
    "    lambda o: img2lrgbboxscaled[o][1]\n",
    "]\n",
    "k, getters[0](k), getters[1](k), getters[2](k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6dcb15",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Now we have more than two blocks in the model we need to specify `n_inp = 1` so FastAI can understand that there is one input, and two output blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6502979d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "dblock = DataBlock(\n",
    "    blocks=(ImageBlock, RegressionBlock(n_out=4), CategoryBlock),\n",
    "    getters=getters,\n",
    "    item_tfms=item_tfms,\n",
    "    n_inp=1\n",
    ")\n",
    "dls = dblock.dataloaders(imgs, bs=128); len(dls.vocab), dls.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ac9bcb",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We can view the batches, as in the last part we use a custom function to see the bounding boxes. Note that when using the proper FastAI API this won't be necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cbf731",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def bb_hw(a): return np.array([a[0],a[1],a[2]-a[0]+1,a[3]-a[1]+1])\n",
    "\n",
    "def show_batch(dls):\n",
    "    b = dls.one_batch()\n",
    "    #print(b[2])\n",
    "\n",
    "    axs = subplots(3,3)[1].flat\n",
    "    for img,box,c,ax in zip(b[0][:9],b[1][:9],b[2],axs):\n",
    "        show_img(img,ax=ax)\n",
    "        label = dls.vocab[c]\n",
    "        draw_rect(ax,bb_hw(box.tolist()),color=colr_list[0])\n",
    "        draw_text(ax,bb_hw(box.tolist())[:2],label,color=colr_list[0])\n",
    "\n",
    "show_batch(dls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5f600e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We will continue to use the general `resnet34` backbone, but at this point the `vision_learner` API is not able to infer an appropriate head on its own. We will define a custom head, this is very similar to the ones we have seen before but it just needs to output the appropriate number of activations, in this case four for the bounding box and one for each possible class label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dc7d8e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "head_reg4 = nn.Sequential(\n",
    "    Flatten(),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(25088, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(256),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(256, 4+len(dls.vocab))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e1a1ea",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Clearly now some of the output activations are predicting the box, and some are predicting the class label. The Loss function, not the model architecture, defines how the activations are used and will have to discriminate between those used for the bounding box and those used for the class label. The former is a regression problem, which can use an L1 loss measure. The class is a single label classification suitable for cross entropy. As the loss function needs to return a sclar we just sum these two values. Note that because the cross entropy loss is on a different scale we multiply the cross entropy loss by a scaling factor. This is a hyperparameter which can be tuned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38432c8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "CROSS_ENTROPY_SCALE = 20\n",
    "\n",
    "def detn_loss(input, bb_t, c_t):\n",
    "    # Separate\n",
    "    bb_i, c_i = input[:, :4], input[:, 4:]\n",
    "    # Scale the bounding box on [0, 1] and then multiply by the image size\n",
    "    bb_i = F.sigmoid(bb_i)*SIZE\n",
    "\n",
    "    return F.l1_loss(bb_i, bb_t) + F.cross_entropy(c_i, c_t) * CROSS_ENTROPY_SCALE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c99a80",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "It also helps to have some custom metrics. Remember the loss is a function for the  computer, and the metric is a function for the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa001edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detn_l1(input, bb_t, c_t):\n",
    "    bb_i = input[:, :4]\n",
    "    bb_i = F.sigmoid(bb_i) * SIZE\n",
    "    return F.l1_loss(bb_i, bb_t).data\n",
    "\n",
    "def detn_ce(input, bb_t, c_t):\n",
    "    c_i = input[:, 4:]\n",
    "    return F.cross_entropy(c_i, c_t).data\n",
    "\n",
    "def detn_acc(input, bb_t, c_t):\n",
    "    c_i = input[:, 4:]\n",
    "    return accuracy(c_i, c_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae7ae84",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Now we can create the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c7521e",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = vision_learner(dls, resnet34, loss_func=detn_loss, custom_head=head_reg4, metrics=[detn_l1, detn_ce, detn_acc])\n",
    "head = learn.model[1]; head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bd32e8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Training\n",
    "As before we find an appropriate learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea96eaa",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "lrs = learn.lr_find()\n",
    "lrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b5f282",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "And train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c37f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fine_tune(20, base_lr=lrs.valley)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae8fe91",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Analysis\n",
    "\n",
    "Let's check out some predictions to see how we did. As before we make some helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc33170",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Some functions to visulize the predictions of our model.\n",
    "def bb_hw(a): return np.array([a[0],a[1],a[2]-a[0]+1,a[3]-a[1]+1])\n",
    "\n",
    "def show_preds(learner,valid=True):\n",
    "    dl = learn.dls[1 if valid else 0].new(shuffle=True)\n",
    "    imgs,boxes,labels = dl.one_batch()\n",
    "    learner.model.eval()\n",
    "    preds = learner.model(imgs)\n",
    "\n",
    "    fig,axs = subplots(9,2)\n",
    "    for img,box,label,pred,ax in zip(imgs,boxes,labels,preds,axs):\n",
    "        img = (img-img.min())/(img.max()-img.min())\n",
    "        bb_pred = F.sigmoid(pred[:4])*224\n",
    "        cl_pred = pred[4:]\n",
    "        cl_pred = cl_pred.argmax()\n",
    "        show_img(img,ax=ax[0])\n",
    "        draw_rect(ax[0],bb_hw(box.tolist()),color=colr_list[0])\n",
    "        draw_text(ax[0],bb_hw(box.tolist())[:2],learner.dls.vocab[label],color=colr_list[0])\n",
    "        show_img(img,ax=ax[1])\n",
    "        draw_rect(ax[1],bb_hw(bb_pred.tolist()),color=colr_list[0])\n",
    "        draw_text(ax[1],bb_hw(bb_pred.tolist())[:2],learner.dls.vocab[cl_pred],color=colr_list[0])\n",
    "    fig.tight_layout():\n",
    "    axs[0][0].set_title('label')\n",
    "    axs[0][1].set_title('prediction')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84c5431",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "show_preds(learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d8effc",
   "metadata": {},
   "source": [
    "The groundtruth is on the left, and the predictions on the right."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
