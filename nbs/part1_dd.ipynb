{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ae3b332",
   "metadata": {},
   "source": [
    "# Defect Detection from Scratch\n",
    "\n",
    "In this and subsequent notebooks we are going to incrementally construct a defect detection model using PyTorch and the current version of FastAI. The approach will be single pass and efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679c47e6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "In these initial notebooks we are going to use a dataset containing images of defects in aircraft glass (AGDD). Each sample consists of two images of the same sample area, one with front-side and one with backside illumination. This is a reasonable starting point, as it allows us to understand how to load multiple images into the model.\n",
    "\n",
    "The notebooks will build from scratch, in this first pass we detect only the bounding box of the largest defect using the front-side illumination. Later notebooks will infer the categories, multiple defects, and use both input images.\n",
    "\n",
    "This first cell downloads the data. Only run it once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145bf25e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import fsspec\n",
    "from pathlib import Path\n",
    "\n",
    "path = Path.cwd() / \"data\"\n",
    "\n",
    "if not path.exists():\n",
    "    path.mkdir(exist_ok=True, parents=True)\n",
    "    fs = fsspec.filesystem(\"github\", org=\"core128\", repo=\"AGDD\")\n",
    "    \n",
    "    for subfolder in [\"image\", \"images\", \"labels_rect\"]:\n",
    "        for split in [\"train\", \"val\"]:\n",
    "            destination = path / subfolder / split\n",
    "            destination.mkdir(exist_ok=True, parents=True)\n",
    "            fs.get(fs.ls(f\"data/{subfolder}/{split}\"), destination.as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24967fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff0f87b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Exploring\n",
    "The data is in two subfolders. The `images` subfolder contains the back-side illuminated images, the `image` folder the frontside illuminated image and the `labels_rect` contains the labels. Each of these folders contains a `train` and `val` subfolder: the data is pre-split for us.\n",
    "\n",
    "The first job is to get the data into an appropriate form. Generally vision problems like this use data in a COCO format, so it would be sensible to try and convert the data into a similar format. Then we can use the usual FastAI processing pipelines. We can create this JSON for the training and validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266251d0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "labels_dir = path / 'labels_rect'\n",
    "\n",
    "def box_cxcywh_to_xywh(x):\n",
    "    x_c, y_c, w, h = x[0], x[1], x[2], x[3]\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "         w, h]\n",
    "    return np.array(b)\n",
    "\n",
    "\n",
    "def create_coco_format_json(classes, filepaths):\n",
    "    \"\"\"\n",
    "    This function creates a COCO dataset.\n",
    "    :param classes: list of strings where each string is a class.\n",
    "    :param filepaths: a list of strings containing all images paths\n",
    "    :return dataset_coco_format: COCO dataset (JSON).\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    annotations = []\n",
    "    categories = []\n",
    "    count = 0\n",
    "    \n",
    "    # Creates a categories list, i.e: [{'id': 0, 'name': 'a'}, {'id': 1, 'name': 'b'}, {'id': 2, 'name': 'c'}] \n",
    "    for idx, class_ in enumerate(classes):\n",
    "        categories.append(\n",
    "            { \n",
    "                \"id\": idx,\n",
    "                \"name\": class_\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    # Iterate over image filepaths\n",
    "    for each in filepaths:\n",
    "        # Get the image id, e.g: \"10044\"\n",
    "        file_name = each.name\n",
    "        file_id = Path(file_name).stem\n",
    "        image = Image.open(each)\n",
    "        height, width = image.shape\n",
    "\n",
    "        images.append(\n",
    "            {\n",
    "                \"id\": file_id,\n",
    "                \"width\": width,\n",
    "                \"height\": height,\n",
    "                \"file_name\": file_name,\n",
    "                \"date_captured\": \"2013-11-18 02:53:27\"\n",
    "            }\n",
    "        )\n",
    "\n",
    "        parent = each.parent.name\n",
    "        label_path = labels_dir / parent / f\"{file_id}.txt\"\n",
    "        labels = np.genfromtxt(label_path)\n",
    "\n",
    "\n",
    "        # If there are labels:\n",
    "        if labels.size != 0:\n",
    "            # expand the dim so we can iterate\n",
    "            if labels.ndim == 1:\n",
    "                labels = np.expand_dims(labels, axis=0)\n",
    "\n",
    "\n",
    "            for each in labels:\n",
    "                bb = box_cxcywh_to_xywh(each[1:])\n",
    "                # Scale to the image size\n",
    "                bb[0] *= width\n",
    "                bb[2] *= width\n",
    "                bb[1] *= height\n",
    "                bb[3] *= height\n",
    "                seg = {\n",
    "                    'bbox': bb.astype(int).tolist(),\n",
    "                    'image_id':file_id, \n",
    "                    'category_id': each[0],\n",
    "                    'id': count\n",
    "                }\n",
    "                annotations.append(seg)\n",
    "                count += 1\n",
    "\n",
    "\n",
    "\n",
    "    # Create the dataset\n",
    "    dataset_coco_format = {\n",
    "        \"categories\": categories,\n",
    "        \"images\": images,\n",
    "        \"annotations\": annotations,\n",
    "    }\n",
    "    \n",
    "    return dataset_coco_format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44276497",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "This code creates the COCO format json for the training and validation sets for both illuminations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a684e11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "classes = [\"contusion\", \"scratch\", \"crack\", \"spot\"]\n",
    "\n",
    "for subdir in ['image', 'images']:\n",
    "    subdir = path / subdir\n",
    "    for splitdir in ['train', 'val']:\n",
    "        splitdir = subdir / splitdir\n",
    "        image_paths = splitdir.glob('**/*.png')\n",
    "        coco = create_coco_format_json(classes, image_paths)\n",
    "        with open(splitdir/'data.json', 'w') as f:\n",
    "            json.dump(coco, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894be211",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "FastAI has a `get_annotations` function which can parse COCO style bounding boxes similar to those in the dataset. Here we just load the data associated with the training set. This function returns a tuple of lists, the first containing the image file names in the training set and the second the corresponding bounding boxes and categories. We can check the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd54d7e7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "imgs, lbl_bbox = get_annotations(path/'image'/'train'/'data.json')\n",
    "len(imgs), len(lbl_bbox), imgs[1], lbl_bbox[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbd44c2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "And the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3721437",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "val_imgs, val_lbl_bbox = get_annotations(path/'image'/'val'/'data.json')\n",
    "len(val_imgs), len(val_lbl_bbox), val_imgs[1], val_lbl_bbox[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2807c6f0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32cc9fa8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "The bounding box contains four numbers, the first pair are the xy coordinates of the upper left corner and the second are those of the lower left corner of the box.\n",
    "\n",
    "# Visualisation\n",
    "We can use `matplotlib` to visualise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5150c6a9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cmx\n",
    "from matplotlib import patches, patheffects\n",
    "\n",
    "img_file, img_bbox = imgs[1], lbl_bbox[1]\n",
    "img = Image.open(path/'image'/'train'/img_file)\n",
    "h, w = img.shape\n",
    "h, w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fec2fc",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "These images are square, and uniform so we can probably proceed as is. For clarity though we define the size: all the images to be ingested have to be uniform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70580da2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "SIZE = 640\n",
    "\n",
    "img_scaled = img.resize((640, 640))\n",
    "img_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98118e0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "In object detection the independent variable is the image, and the dependent ones are the classes and bounding boxes. Given an image we want to predict a class label for each object present in the image, in addition to a bounding box for each. As the bounding box is defined in the coordinate space of the image (independent) scaling must be applied consistently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2fe7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "xscale, yscale = w/SIZE, h/SIZE\n",
    "img_bbox_scaled = [[x1//xscale, y1//yscale, x2//xscale, y2//yscale] for x1, y1, x2, y2 in img_bbox[0]]\n",
    "img_bbox_scaled = (img_bbox_scaled, img_bbox[1])\n",
    "img_bbox_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff80339",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "This is a small utility function to display an image with some overlaid annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd65ca41",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def show_img(im, figsize=None, ax=None):\n",
    "    if not ax: fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.imshow(im)\n",
    "    ax.set_xticks(np.linspace(0, SIZE, 8))\n",
    "    ax.set_yticks(np.linspace(0, SIZE, 8))\n",
    "    ax.grid()\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    return ax\n",
    "\n",
    "show_img(img_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b93c84f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We want to overlay the class labels and bounding boxes in a useful way. We can use the utility functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cace8b55",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# draw an outline around the shape; used to add contrast to the text so we can read it easily\n",
    "def draw_outline(o, lw):\n",
    "    o.set_path_effects([patheffects.Stroke(\n",
    "        linewidth=lw, foreground='black'), patheffects.Normal()])\n",
    "\n",
    "# draw text in the specified location along with an outline so that there is some contrast between the text and the image\n",
    "def draw_text(ax, xy, txt, sz=14, color='white'):\n",
    "    text = ax.text(*xy, txt,\n",
    "        verticalalignment='top', color=color, fontsize=sz, weight='bold')\n",
    "    draw_outline(text, 1)\n",
    "\n",
    "def draw_rect(ax, b, color='white'):\n",
    "    patch = ax.add_patch(patches.Rectangle(b[:2], *b[-2:], fill=False, edgecolor=color, lw=2))\n",
    "    draw_outline(patch, 4)\n",
    "\n",
    "def get_cmap(N):\n",
    "    color_norm  = mcolors.Normalize(vmin=0, vmax=N-1)\n",
    "    return cmx.ScalarMappable(norm=color_norm, cmap='Set3').to_rgba\n",
    "\n",
    "# generate a list of different colors for rendering our bounding boxes\n",
    "num_colr = 12\n",
    "cmap = get_cmap(num_colr)\n",
    "colr_list = [cmap(float(x)) for x in range(num_colr)]\n",
    "\n",
    "# draw an image along with it's associated bounding boxes and class labels\n",
    "def show_item(im, lbl_bbox, figsize=None, ax=None):\n",
    "    if not ax: fig,ax = plt.subplots(figsize=figsize)\n",
    "    ax = show_img(im, ax=ax)\n",
    "    for i,(b,c) in enumerate(zip(lbl_bbox[0], lbl_bbox[1])):\n",
    "        b = (*b[:2],b[2]-b[0]+1,b[3]-b[1]+1)\n",
    "        draw_rect(ax, b, color=colr_list[i%num_colr])\n",
    "        draw_text(ax, b[:2], c, color=colr_list[i%num_colr])\n",
    "\n",
    "show_item(img_scaled,img_bbox_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06be8e35",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Decomposition\n",
    "Let's start with a nice simple problem. We can create a model which takes an image as input and predicts one object class. We can do this for the *largest* object present in the image. The dataset and bounding box information can give us this data, and we can use the derived dataset.\n",
    "\n",
    "This function takes a labelled bounding box sample and returns the largest single bounding box along with its class label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38268dd2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def area(box): return (box[2] - box[0]) * (box[3] - box[1])\n",
    "\n",
    "def get_largest(boxes):\n",
    "    return sorted(L(zip(*boxes)), key=lambda each: -area(each[0]))[0]\n",
    "\n",
    "img_bbox_scaled, get_largest(img_bbox_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c4cc81",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Now we can use a list comprehension to make a new training set with only the largest object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ae9fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrg_bbox = [get_largest(boxes) for boxes in lbl_bbox]\n",
    "img2lrgbbox = dict(zip(imgs, lrg_bbox))\n",
    "k = L(img2lrgbbox)[1]; k, img2lrgbbox[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d1070a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Training a Classifier\n",
    "We begin by creating a dataloader. To do this we want to merge the validation and training sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784d3f4c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "all_imgs = [f\"train/{each}\" for each in imgs] + [f\"val/{each}\" for each in val_imgs]\n",
    "all_lbl_bbox = lbl_bbox + val_lbl_bbox\n",
    "all_imgs, all_lbl_bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc18254a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "And of course we need to remake the largest item data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd0a269",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lrg_bbox = [get_largest(boxes) for boxes in all_lbl_bbox]\n",
    "allimg2lrgbbox = dict(zip(all_imgs, all_lrg_bbox))\n",
    "k = L(allimg2lrgbbox)[1]; k, allimg2lrgbbox[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6614a656",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "The getters are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e11384",
   "metadata": {},
   "outputs": [],
   "source": [
    "getters = [lambda o: path/'image'/o, lambda o: allimg2lrgbbox[o][1]]\n",
    "item_tfms = [Resize(SIZE, method='squish')]\n",
    "batch_tfms = [Rotate(10), Flip(), Dihedral()]\n",
    "dblock = DataBlock(blocks=(ImageBlock(cls=PILImageBW), CategoryBlock),\n",
    "                   getters=getters,\n",
    "                   item_tfms=item_tfms,\n",
    "                   batch_tfms=batch_tfms,\n",
    "                   splitter = FuncSplitter(lambda o: Path(o).parent == 'val'))\n",
    "dls = dblock.dataloaders(all_imgs, bs=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b1f0fa",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "FastAI looks at the dataset, and collects all the classes into a vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d093c9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.vocab, len(dls.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029c62e2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "And we can inspect a batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088c021e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cba4b6e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Now we can use the usual vision learner API, specifying the dataset, architecture, loss and metrics. Here we choose a `resnet34` which is balanced between capacity and performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3881f9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = vision_learner(dls, resnet34, metrics=[accuracy, error_rate])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee94cb90",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Look at the backbone;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54455ea3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "backbone = learn.model[0]; backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59c51b2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "FastAI has selected the cross entropy loss. We can find a learning rate suitable for the problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc65206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = learn.lr_find()\n",
    "lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4710ccf",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Training\n",
    "Now we can fit using the selected loss function. Note that we use `fine_tune` here as the `resnet34` has pretrained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a531ed",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "learn.fine_tune(10, base_lr=lr.valley)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60577bd4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "learn.show_results(dl=dls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64562ee3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "These results seem quite bad! We need to work on this... It could be because we are only using one image as input. Something to work on! Let's try again with a composite fusion:\n",
    "\n",
    "# Composite Images\n",
    "If we want to make a composite model, the simplest thing is to average all the images. Making a model to ingest two image inputs is more complex, and will be covered at a later date. We make a three-channel image, where the first contains the average, the second contains the second image and the third contains the first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243e931b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import shutil\n",
    "\n",
    "\n",
    "destination = Path.cwd() / \"data\" / \"composite\"\n",
    "destination.mkdir(exist_ok=True, parents=True)\n",
    "(destination / \"train\").mkdir(exist_ok=True, parents=True)\n",
    "(destination / \"val\").mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "shutil.copyfile(path / \"image\" / \"train\" / \"data.json\", destination / \"train\" / \"data.json\")\n",
    "shutil.copyfile(path / \"image\" / \"val\" / \"data.json\", destination / \"val\" / \"data.json\")\n",
    "\n",
    "for each in all_imgs:\n",
    "    first = path / \"image\" / each\n",
    "    second = path / \"images\" / each\n",
    "    destination = path / \"composite\" / each\n",
    "\n",
    "    first = cv2.imread(first, cv2.IMREAD_GRAYSCALE)\n",
    "    second = cv2.imread(second, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    avg_value = (first.astype(int) + second.astype(int)) // 2\n",
    "    avg_value = avg_value.astype('uint8')\n",
    "\n",
    "    bgr_img = cv2.merge((avg_value, second, first))\n",
    "    cv2.imwrite(destination, bgr_img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0042fc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "getters = [\n",
    "    lambda o: path/'composite'/o, \n",
    "    lambda o: allimg2lrgbbox[o][1]\n",
    "]\n",
    "item_tfms = [Resize(SIZE, method='squish')]\n",
    "batch_tfms = [Rotate(10), Flip(), Dihedral()]\n",
    "dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n",
    "                   n_inp=1,\n",
    "                   getters=getters,\n",
    "                   item_tfms=item_tfms,\n",
    "                   batch_tfms=batch_tfms,\n",
    "                   splitter = FuncSplitter(lambda o: Path(o).parent == 'val'))\n",
    "dls = dblock.dataloaders(all_imgs, bs=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e7f254",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "FastAI looks at the dataset, and collects all the classes into a vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66031c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.vocab, len(dls.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592b11dd",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "And we can inspect a batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0ac573",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def show_batch(dls):\n",
    "    b = dls.one_batch()\n",
    "    print(len(b), b[0].shape)\n",
    "\n",
    "    axs = subplots(3, 3)[1].flat\n",
    "    for img, box, ax in zip(b[0][:9], b[1][:9], axs):\n",
    "        show_img(img, ax=ax)\n",
    "\n",
    "show_batch(dls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fb1719",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Now we can use the usual vision learner API, specifying the dataset, architecture, loss and metrics. Here we choose a `resnet34` which is balanced between capacity and performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22960a4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "learn = vision_learner(dls, resnet34, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453ac7d2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Look at the backbone;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4399582",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "backbone = learn.model[0]; backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35e03c7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "FastAI has selected the cross entropy loss. We can find a learning rate suitable for the problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c8b377",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = learn.lr_find()\n",
    "lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e5c309",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Training\n",
    "Now we can fit using the selected loss function. Note that we use `fine_tune` here as the `resnet34` has pretrained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a924f44",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "learn.fine_tune(10, base_lr=lr.valley)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c63ed9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "learn.show_results(dl=dls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c771d9ae",
   "metadata": {},
   "source": [
    "This seems like a better result!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
